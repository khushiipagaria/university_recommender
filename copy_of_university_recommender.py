# -*- coding: utf-8 -*-
"""Copy of university_recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1acAFqv8Q-jW7O53brBNddM0THrZEaVzW
"""

import sys
import os
import collections
from collections import defaultdict
import numpy as np
import pandas as pd
from scipy import stats
import re

cs_file = "/content/Final.csv"
data = pd.read_csv(cs_file)
data.shape

data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
data.head()

data.columns = ['univName', 'major', 'program', 'season', 'decision', 'Method', 'decdate', 'decdate_ts', 'cgpa', 'greV', 'greQ',
           'greA', 'is_new_gre', 'gre_subject','status', 'post_data', 'post_timestamp', 'comments']
data.head()

data = data[data['decision'] =='Accepted']

data.shape

data = data[pd.notnull(data['greQ'])]
data.shape

data['greQ'] = data['greQ'].fillna(130)
data['greV'] = data['greV'].fillna(130)
data['greA'] = data['greA'].fillna(0)
data.greA.head()

uni_names = data['univName'].unique()

similar_univs = pd.DataFrame({'univName':uni_names})
similar_univs

data.describe()

def convert_quant_score(quant_score):
    quant_list = []
    quant_score = quant_score.tolist()
    for old_quant in quant_score:
        if old_quant <= 170:
            quant_list.append(old_quant)
            continue
        else:
            old_quant = old_quant/4.7
            if old_quant <=130:
                quant_list.append(130)
            else:
                quant_list.append(old_quant)
    return quant_list

def convert_verbal_score(verbal_score):
    verbal_list = []
    verbal_score = verbal_score.tolist()
    for old_verbal in verbal_score:
        if old_verbal <= 170:
            verbal_list.append(old_verbal)
            continue
        else:
            old_verbal = old_verbal/4.7
            if old_verbal <=130:
                verbal_list.append(130)
            else:
                verbal_list.append(old_verbal)
    return verbal_list

data['greQ'] = convert_quant_score(data['greQ'])
data['greV'] = convert_verbal_score(data['greV'])

import matplotlib.pyplot as plt
import seaborn as sns

sns.pairplot(data, palette="husl", x_vars=["greV","cgpa","greQ"], y_vars=["greV","cgpa","greQ"], height=8)
plt.show()

def normalize_gpa(data2,cgpa,totalcgpa):
    cgpa = data2[cgpa].tolist()
    totalcgpa = data2[totalcgpa].tolist()
    for i in range(len(cgpa)):
        if totalcgpa[i] != 0:
            cgpa[i] = cgpa[i] / totalcgpa[i]
        else:
            cgpa[i] = 0
    data2['cgpa'] = cgpa
    return data2

data = data.drop(columns='major')
data = data.drop(columns='program')
data = data.drop(columns='season')
data = data.drop(columns='decision')
data = data.drop(columns='Method')
data = data.drop(columns='decdate')
data = data.drop(columns='decdate_ts')
data = data.drop(columns='is_new_gre')
data = data.drop(columns='gre_subject')
data = data.drop(columns='status')
data = data.drop(columns='post_data')
data = data.drop(columns='post_timestamp')
data = data.drop(columns='comments')

university_list = list(set(data['univName'].tolist()))
for i in range(len(university_list)):
	if(len(data[data['univName'] == university_list[i]]) < 100):
		data = data[data['univName'] != university_list[i]]
data = data.dropna()

data.head()

import math
from sklearn import neighbors, datasets
from numpy.random import permutation
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_fscore_support

processed_data = data[['greV', 'greQ', 'greA', 'cgpa', 'univName']]
processed_data.head()
processed_data.to_csv('/content/Processed_data.csv')

data.head()

#similar_univs = pandas.read_csv('similar_universities.csv')
random_indices = permutation(data.index)
test_cutoff = math.floor(len(data)/5)
print(test_cutoff)
test = processed_data.loc[random_indices[1:test_cutoff]]
train = processed_data.loc[random_indices[test_cutoff:]]
train_output_data = train['univName']
print("train Output data", train_output_data)
train_input_data = train
train_input_data = train_input_data.drop(columns='univName')
print("train input data", train_input_data)
test_output_data = test['univName']
print("test Output data", test_output_data)
test_input_data = test
test_input_data = test_input_data.drop(columns='univName')
print("test input data", test_input_data)

def euclideanDistance(data1, data2, length):
    distance = 0
    for x in range(length):
        distance += np.square(data1[x] - data2[x])
    return np.sqrt(distance)

def knn(trainingSet, testInstance, k):
    print(k)
    distances = {}
    sort = {}
    length = testInstance.shape[1]

    for x in range(len(trainingSet)):

        dist = euclideanDistance(testInstance, trainingSet.iloc[x], length)

        distances[x] = dist[0]

    sorted_d = sorted(distances.items(), key=lambda x: x[1])


    neighbors = []

    for x in range(k):
        neighbors.append(sorted_d[x][0])

    classVotes = {}

    for x in range(len(neighbors)):
        response = trainingSet.iloc[neighbors[x]][-1]
        if response in classVotes:
            classVotes[response] += 1
        else:
            classVotes[response] = 1

    sortedVotes = sorted(classVotes.items(), key=lambda x: x[1], reverse=True)

    return(sortedVotes, neighbors)

testSet = [[142, 153, 5.0, 3.6]]
test = pd.DataFrame(testSet)
test.shape

k = 7

result,neigh= knn(processed_data, test, k)


list1 = []
list2 = []
for i in result:
    list1.append(i[0])
    list2.append(i[1])
for i in list1:
    print(i)

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=5)
neigh.fit(processed_data.iloc[:,0:4], data['univName'])

print(neigh.predict(test))

import joblib

# Save the processed data
processed_data.to_csv('/content/Processed_data.csv', index=False)

# Save the trained KNN model
joblib.dump(neigh, '/content/knn_model.pkl')

import joblib
joblib.dump(neigh, 'knn_model.pkl')

from google.colab import files
files.download('knn_model.pkl')

!pip install flask flask-ngrok pandas scikit-learn joblib

!mkdir -p /content/templates

!ls /content/templates

!mkdir -p /content/app.py

from flask import Flask, request, render_template
from flask_ngrok import run_with_ngrok  # Only for Google Colab
import pandas as pd
import joblib

# Initialize Flask app
app = Flask(__name__)
run_with_ngrok(app)  # Use this only in Colab

# Load the model
model = joblib.load('/content/knn_model.pkl')  # Update path if needed

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/recommend', methods=['POST'])
def recommend():
    try:
        # Get data from the form
        greV = float(request.form['greV'])
        greQ = float(request.form['greQ'])
        greA = float(request.form['greA'])
        cgpa = float(request.form['cgpa'])

        # Prepare input for the model
        input_data = [[greV, greQ, greA, cgpa]]
        input_df = pd.DataFrame(input_data, columns=['greV', 'greQ', 'greA', 'cgpa'])

        # Make a prediction
        prediction = model.predict(input_df)[0]
        return f"<h2>Recommended University: {prediction}</h2>"
    except Exception as e:
        return f"<h2>Error: {str(e)}</h2>"

if __name__ == '__main__':
    app.run()  # Only use app.run() without additional arguments when using run_with_ngrok

from flask import Flask, render_template, request
import joblib

app = Flask(__name__)

# Load your trained model
model = joblib.load('knn_model.pkl')
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001, debug=True)


@app.route('/')
def home():
    return render_template('index.html')

@app.route('/recommend', methods=['POST'])
def recommend():
    # Handle recommendation logic here
    greV = float(request.form['greV'])
    greQ = float(request.form['greQ'])
    greA = float(request.form['greA'])
    cgpa = float(request.form['cgpa'])

    prediction = model.predict([[greV, greQ, greA, cgpa]])
    return render_template('index.html', prediction=prediction[0])

if __name__ == '__main__':
    app.run(port=5001)

